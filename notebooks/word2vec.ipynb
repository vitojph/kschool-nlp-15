{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplo de `word2vec` con `gensim`\n",
    "\n",
    "\n",
    "En la siguiente celda, importamos las librerías necesarias y configuramos los mensajes de los logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim, logging, os\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento de un modelo\n",
    "\n",
    "Implemento una clase `Corpus` con un iterador sobre un directorio que contiene ficheros de texto. Utilizaré una instancia de `Corpus` para poder procesar de manera más eficiente una colección, sin necesidad de cargarlo previamente en memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus(object):\n",
    "    '''Clase Corpus que permite leer de manera secuencial un directorio de documentos de texto'''\n",
    "    \n",
    "    def __init__(self, directorio):\n",
    "        self.directory = directorio\n",
    "\n",
    "    def __iter__(self):\n",
    "        for fichero in os.listdir(self.directory):\n",
    "            for linea in open(os.path.join(self.directory, fichero)):\n",
    "                yield linea.split()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CORPUSDIR` contiene una colección de noticias en español (normalizada previamente a minúsculas y sin signos de puntuación) con alrededor de 150 millones de palabras. Entrenamos un modelo en un solo paso, ignorando aquellos tokens que aparecen menos de 10 veces (ignorando erratas, básicamente), para construir vectores de 200 dimensiones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CORPUSDIR = 'PATH_TO_YOUR_CORPUS_DIRECTORY'\n",
    "oraciones = Corpus(CORPUSDIR)\n",
    "model = gensim.models.Word2Vec(oraciones, min_count=10, size=200, workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez completado el entrenamiento (después de casi 30 minutos), guardamos el modelo en disco. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('PATH_TO_YOUR_MODEL.w2v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el futuro, podremos utilizar este modelo cargándolo en memoria con la instrucción:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec.load('/data/w2v/eswiki-280.w2v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probando nuestro modelo\n",
    "\n",
    "El objeto `model` contiene una enorme matriz de números: una tabla, donde cada fila es uno de los términos del vocabulario reconocido y cada columna es una de las características que permiten modelar el significado de dicho término.\n",
    "\n",
    "En nuestro modelo, tal y como está entrenado, tenemos más de 26 millones de términos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.corpus_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada término del vocabulario está representado como un vector con 150 dimensiones: 105 características. Podemos acceder al vector de un término concreto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model['azul'], '\\n')\n",
    "\n",
    "print(model['verde'], '\\n')\n",
    "\n",
    "print(model['microsoft'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estos vectores no nos dicen mucho, salvo que contienen números muy pequeños :-/\n",
    "\n",
    "El mismo objeto `model` permite acceder a una serie de funcionalidades ya implementadas que nos van a permitir evaluar formal e informalmente el modelo. Por el momento, nos contentamos con los segundo: vamos a revisar visualmente los significados que nuestro modelo ha aprendido por su cuenta. \n",
    "\n",
    "Podemos calcular la similitud semántica entre dos términos usando el método `similarity`, que nos devuelve un número entre 0 y 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('hombre - mujer', model.wv.similarity('hombre', 'mujer'))\n",
    "\n",
    "print('perro - gato', model.wv.similarity('perro', 'gato'))\n",
    "\n",
    "print('gato - periódico', model.wv.similarity('gato', 'periódico'))\n",
    "\n",
    "print('febrero - azul', model.wv.similarity('febrero', 'azul'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos seleccionar el término que no encaja a partir de una determinada lista de términos usando el método `doesnt_match`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista1 = 'madrid barcelona gonzález washington'.split()\n",
    "print('en la lista', ' '.join(lista1), 'sobra:', model.wv.doesnt_match(lista1))\n",
    "\n",
    "lista2 = 'psoe pp ciu ronaldo'.split()\n",
    "print('en la lista', ' '.join(lista2), 'sobra:', model.wv.doesnt_match(lista2))\n",
    "\n",
    "lista3 = 'publicaron declararon soy negaron'.split()\n",
    "print('en la lista', ' '.join(lista3), 'sobra:', model.wv.doesnt_match(lista3))\n",
    "\n",
    "lista4 = 'homero saturno cervantes shakespeare cela'.split()\n",
    "print('en la lista', ' '.join(lista4), 'sobra:', model.wv.doesnt_match(lista4))\n",
    "\n",
    "lista5 = 'madrid barcelona alpedrete marsella'.split()\n",
    "print('en la lista', ' '.join(lista5), 'sobra:', model.wv.doesnt_match(lista5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos buscar los términos más similares usando el método `most_similar` de nuestro modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminos = 'psoe chicago rajoy enero amarillo microsoft iberia ronaldo messi atlético 1992'.split()\n",
    "\n",
    "for t in terminos:\n",
    "    print(t, '==>', model.wv.most_similar(t), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el mismo método `most_similar` podemos combinar vectores de palabras tratando de jugar con los rasgos semánticos de cada una de ellas para descubrir nuevas relaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mujer que ejerce la autoridad en una alcaldía ==> alcalde + mujer - hombre')\n",
    "most_similar = model.wv.most_similar(positive=['alcalde', 'mujer'], negative=['hombre'], topn=3)\n",
    "for item in most_similar:\n",
    "    print(item)\n",
    "\n",
    "print('monarca soberano ==> reina + hombre - mujer')    \n",
    "most_similar = model.wv.most_similar(positive=['reina', 'hombre'], negative=['mujer'], topn=3)\n",
    "for item in most_similar:\n",
    "    print(item)\n",
    "    \n",
    "print('capital de Alemania ==> moscú + alemania - rusia')\n",
    "most_similar = model.wv.most_similar(positive=['moscú', 'alemania'], negative=['rusia'], topn=3)\n",
    "for item in most_similar:\n",
    "    print(item)\n",
    "\n",
    "print('presidente de Francia ==> rajoy + francia - españa')\n",
    "most_similar = model.wv.most_similar(positive=['mariano', 'francia'], negative=['españa'], topn=3)\n",
    "for item in most_similar:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar = model.wv.most_similar(positive=['obama', 'francia'], negative=['eeuu'], topn=3)\n",
    "for item in most_similar:\n",
    "    print(item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
